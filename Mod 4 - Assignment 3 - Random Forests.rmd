## Random Forests

```{r, include = FALSE}
library(titanic)
library(tidyverse)
library(tidymodels)
library(mice) #package for imputation
library(VIM) #visualizing missingness
library(ranger) #for random forests
library(randomForest) #also for random forests
library(caret)
library(gridExtra)
library(mice) #package for imputation
library(VIM) #visualizing missingness
library(naniar) #visualizing missingness
library(skimr) #alternative way to view dataset summaries
library(UpSetR) #visualizing missingness
library(vip) #variable importance
```

```{r}
library(readr)
drug <- read_csv("drug_data-2.csv")
```

```{r}
names(drug) = c("ID", "Age", "Gender", "Education", "Country", "Ethnicity",
"Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive",
"SS", "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis",
"Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh",
"LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA")

```

```{r}
drug[drug == "CL0"] = "No"
drug[drug == "CL1"] = "No"
drug[drug == "CL2"] = "Yes"
drug[drug == "CL3"] = "Yes"
drug[drug == "CL4"] = "Yes"
drug[drug == "CL5"] = "Yes"
drug[drug == "CL6"] = "Yes"

```

```{r}
drug_clean = drug %>% mutate_at(vars(Age:Ethnicity), funs(as_factor)) %>%
mutate(Age = factor(Age, labels = c("18_24", "25_34", "35_44", "45_54",
"55_64", "65_"))) %>%
mutate(Gender = factor(Gender, labels = c("Male", "Female"))) %>%
mutate(Education = factor(Education, labels = c("Under16", "At16", "At17", "At18",
"SomeCollege","ProfessionalCert",
"Bachelors", "Masters",
"Doctorate"))) %>%
mutate(Country = factor(Country, labels = c("USA", "NewZealand", "Other", "Australia",
"Ireland","Canada","UK"))) %>%
mutate(Ethnicity = factor(Ethnicity, labels = c("Black", "Asian", "White",
"White/Black", "Other",
"White/Asian", "Black/Asian"))) %>%
mutate_at(vars(Alcohol:VSA), funs(as_factor)) %>%
dplyr::select(-ID)

```

```{r}
str(drug_clean)

```
```{r}
drug_clean = drug_clean %>% dplyr::select(!(Alcohol:Mushrooms)) %>% dplyr::select(!(Semer:VSA))
```

```{r}

skim(drug_clean)

```


```{r}
set.seed(1234) 
split = initial_split(drug_clean, prop = 0.7, strata = Nicotine) #70% in training
train = training(split) 
test = testing(split)
```

```{r}
# load required libraries
library(ggplot2)
library(gridExtra)

# Age vs Nicotine
p1 <- ggplot(drug_clean, aes(x=Age, fill=Nicotine)) +
  geom_bar(position="fill") +
  labs(title="Age vs Nicotine") +
  theme_minimal()

# Gender vs Nicotine
p2 <- ggplot(drug_clean, aes(x=Gender, fill=Nicotine)) +
  geom_bar(position="fill") +
  labs(title="Gender vs Nicotine") +
  theme_minimal()

# Education vs Nicotine
p3 <- ggplot(drug_clean, aes(x=Education, fill=Nicotine)) +
  geom_bar(position="fill") +
  labs(title="Education vs Nicotine") +
  theme_minimal()

# Country vs Nicotine
p4 <- ggplot(drug_clean, aes(x=Country, fill=Nicotine)) +
  geom_bar(position="fill") +
  labs(title="Country vs Nicotine") +
  theme_minimal()

# Impulsive vs Nicotine
p5 <- ggplot(drug_clean, aes(x=Impulsive, fill=Nicotine)) +
  geom_histogram(data = subset(drug_clean, Nicotine == "Yes"), fill="blue", alpha=0.5, position="identity") +
  geom_histogram(data = subset(drug_clean, Nicotine == "No"), fill="red", alpha=0.5, position="identity") +
  labs(title="Impulsive vs Nicotine") +
  theme_minimal()

# Using grid.arrange to organize these visuals in 2x2 format
grid.arrange(p1, p2, p3, p4, p5, ncol=2, nrow=3)

```

Answers so far:

There is no missingness.

Training set has 1318 rows.

False

True

0.715

SS



Set up our folds for cross-validation  
```{r}
set.seed(123)
rf_folds = vfold_cv(train, v = 5)
```

Random forest with an R-defined tuning grid (this model took about 5 minutes to run)
```{r}
churn_recipe = recipe(Nicotine ~., train) %>%
  step_dummy(all_nominal(), -all_outcomes())

rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% #add tuning of mtry and min_n parameters
  #setting trees to 100 here should also speed things up a bit, but more trees might be better
  set_engine("ranger", importance = "permutation") %>% #added importance metric
  set_mode("classification")

churn_wflow = 
  workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(churn_recipe)

set.seed(123)
rf_res = tune_grid(
  churn_wflow,
  resamples = rf_folds,
  grid = 20 #try 20 different combinations of the random forest tuning parameters
)
```

Look at parameter performance (borrowed from https://juliasilge.com/blog/sf-trees-random-tuning/)
```{r}
rf_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```
Refining the parameters  
```{r}
churn_recipe = recipe(Nicotine ~., train) %>%
  step_dummy(all_nominal(), -all_outcomes())

rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% #add tuning of mtry and min_n parameters
  #setting trees to 100 here should also speed things up a bit, but more trees might be better
  set_engine("ranger", importance = "permutation") %>% #added importance metric
  set_mode("classification")

churn_wflow = 
  workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(churn_recipe)

rf_grid = grid_regular(
  mtry(range = c(2, 8)), #these values determined through significant trial and error
  min_n(range = c(5, 20)), #these values determined through significant trial and error
  levels = 10
)

set.seed(123)
rf_res_tuned = tune_grid(
  churn_wflow,
  resamples = rf_folds,
  grid = rf_grid #use the tuning grid
)
```

```{r}
rf_res_tuned %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```
An alternate view of the parameters  
```{r}
rf_res_tuned %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "Accuracy")
```

```{r}
best_rf = select_best(rf_res_tuned, "accuracy")

final_rf = finalize_workflow(
  churn_wflow,
  best_rf
)

final_rf
```
```{r}
#fit the finalized workflow to our training data
final_rf_fit = fit(final_rf, train)
```

Check out variable importance
```{r}
final_rf_fit %>% pull_workflow_fit() %>% vip(geom = "point")
```

Predictions  
```{r}
trainpredrf = predict(final_rf_fit, train)
head(trainpredrf)
```

Confusion matrix
```{r}
confusionMatrix(trainpredrf$.pred_class, train$Nicotine, 
                positive = "Yes")
```

Predictions on test
```{r}
testpredrf = predict(final_rf_fit, test)
head(testpredrf)
confusionMatrix(testpredrf$.pred_class, test$Nicotine, 
                positive = "Yes")
```


Answers so far:

There is no missingness.

Training set has 1318 rows.

False

True

0.715

SS

train

               Accuracy : 0.8354         

    No Information Rate : 0.6707    

test


                                       
               Accuracy : 0.7002          


overfitting occuring